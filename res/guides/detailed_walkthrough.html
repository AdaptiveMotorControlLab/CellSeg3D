<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Detailed walkthrough &mdash; napari-cellseg3d  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Advanced : Declaring a custom model" href="custom_model_template.html" />
    <link rel="prev" title="Cropping utility guide" href="cropping_module_guide.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html">
            <img src="../../_static/logo_alpha.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Main modules guides:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../welcome.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="review_module_guide.html">Review module guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference_module_guide.html">Inference module guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="training_module_guide.html">Training module guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Utilities :</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="metrics_module_guide.html">Metrics utility guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="convert_module_guide.html">Label conversion utility guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropping_module_guide.html">Cropping utility guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced guides and walk-through:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Detailed walkthrough</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#preparing-images-and-labels">Preparing images and labels</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#cropping">Cropping</a></li>
<li class="toctree-l3"><a class="reference internal" href="#label-conversion-convert-utility">Label conversion : convert utility</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#models-for-object-detection">Models for object detection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#training">Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inference">Inference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#scoring-review-analysis">Scoring, review, analysis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model-performance-metrics-utility">Model performance : Metrics utility</a></li>
<li class="toctree-l3"><a class="reference internal" href="#labels-review">Labels review</a></li>
<li class="toctree-l3"><a class="reference internal" href="#analysis-jupyter-notebooks">Analysis : Jupyter notebooks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="custom_model_template.html">Advanced : Declaring a custom model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Source files:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../code/interface.html">interface.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_base.html">plugin_base.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_review.html">plugin_review.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/launch_review.html">launch_review.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_dock.html">plugin_dock.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_crop.html">plugin_crop.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_convert.html">plugin_convert.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_metrics.html">plugin_metrics.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/model_framework.html">model_framework.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/model_workers.html">model_workers.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/model_instance_seg.html">model_instance_seg.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_model_inference.html">plugin_model_inference.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_model_training.html">plugin_model_training.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/utils.html">utils.py</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">napari-cellseg3d</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Detailed walkthrough</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/res/guides/detailed_walkthrough.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="detailed-walkthrough">
<span id="id1"></span><h1>Detailed walkthrough<a class="headerlink" href="#detailed-walkthrough" title="Permalink to this heading"></a></h1>
<p>The following guide will show you how to use the plugin’s workflow, starting from human-labeled annotation volume, to running inference on novel volumes.</p>
<div class="section" id="preparing-images-and-labels">
<h2>Preparing images and labels<a class="headerlink" href="#preparing-images-and-labels" title="Permalink to this heading"></a></h2>
<p>CellSeg3D was designed for cleared-brain tissue data (collected on mesoSPIM ligthsheet systems). Specifically, we provide a series
of deep learning models that we have found to work well on cortical whole-neuron data. We also provide support for MONAI models, and
we have ported TRAILMAP to PyTorch and trained the model on mesoSPIM collected data. We provide all the tooling for you to use these
weights and also perform transfer learning by fine-tuning the model(s) on your data for even better performance!</p>
<p>To get started with the entire workflow (i.e., fine-tuning on your data), you’ll need at least one pair of image and corresponding labels;
let’s assume you have part of a cleared brain from mesoSPIM imaging as a large .tif file.</p>
<p>If you want to test the models “as is”, please see “Inference” sections in our docs.</p>
<div class="figure align-center" id="id2">
<a class="reference internal image-reference" href="../../_images/init_image_labels.png"><img alt="../../_images/init_image_labels.png" src="../../_images/init_image_labels.png" style="width: 1188.0px; height: 738.0px;" /></a>
<p class="caption"><span class="caption-text">Example of an anisotropic volume (i.e., often times the z resolution is not the same as x and y) and its associated labels.</span><a class="headerlink" href="#id2" title="Permalink to this image"></a></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The approach here will be human-in-the-loop review of labels.
If you need to label your volumes from scratch,
or initially correct labels, please read the Review section right after Cropping.</p>
</div>
<div class="section" id="cropping">
<h3>Cropping<a class="headerlink" href="#cropping" title="Permalink to this heading"></a></h3>
<p>To reduce memory requirements and build a dataset from a single, large volume,
you can use the <strong>cropping</strong> tool to extract multiple smaller images from a large volume for training.</p>
<p>Simply load your image and labels (by checking the “Crop labels simultaneously” option),
and select the volume size you desire to use.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The best way to choose the size when cropping images you intend to use for training models is to use
cubic images sized with a power of two : the default <span class="math notranslate nohighlight">\(64^3\)</span> should be a good start if you’re unsure.
Stick to the same size for a given dataset.
If you simply want to isolate specific regions with variable sizes, you can still compensate for it in training though.
You may also use different image sizes for inference, simply ensure that images in a folder are of a similar size if you
wish to run inference on all of them.</p>
</div>
<p>You can now use the sliders to choose the regions you want to extract,
then either quicksave the results or select the layer you’d like to save and use <strong>CTRL+S</strong> to save it
(useful if you want to name the results or change the file extension).</p>
<div class="figure align-center" id="id3">
<img alt="../../_images/cropping_process_example.png" src="../../_images/cropping_process_example.png" />
<p class="caption"><span class="caption-text">Cropping module layout</span><a class="headerlink" href="#id3" title="Permalink to this image"></a></p>
</div>
</div>
<div class="section" id="label-conversion-convert-utility">
<h3>Label conversion : convert utility<a class="headerlink" href="#label-conversion-convert-utility" title="Permalink to this heading"></a></h3>
<p>Assuming you have instance labels, you’ll need to convert them to semantic labels before using them for training.
To this end, you can use the folder conversion functions in the <em>Convert</em> tab of Utilities :
choose your output directory for the results, select the folder containing your cropped volumes in the <em>Convert folder</em>
section, and click on <strong>“Convert to semantic labels”</strong>.</p>
<div class="figure align-center" id="id4">
<a class="reference internal image-reference" href="../../_images/converted_labels.png"><img alt="../../_images/converted_labels.png" src="../../_images/converted_labels.png" style="width: 680.0px; height: 384.0px;" /></a>
<p class="caption"><span class="caption-text">Example of instance labels from above converted to semantic labels</span><a class="headerlink" href="#id4" title="Permalink to this image"></a></p>
</div>
<p>If you wish to remove small objects, or convert a single image, you can open a single image with <strong>CTRL+O</strong>
and, after selecting the corresponding layer on the left, use the layer-related buttons to perform
your operations.</p>
</div>
</div>
<div class="section" id="models-for-object-detection">
<h2>Models for object detection<a class="headerlink" href="#models-for-object-detection" title="Permalink to this heading"></a></h2>
<div class="section" id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this heading"></a></h3>
<p>If you have a dataset of reasonably sized images (see cropping above) with semantic labels, you’re all set to proceed!
First, load your data by inputting the paths to images and labels, as well as where you want the results to be saved.</p>
<p>There are a few more options on this tab:</p>
<ul>
<li><p>Transfer weights : you can start the model with our pre-trained weights if your dataset comes from cleared brain tissue
imaged by a mesoSPIM (or other lightsheet). If you have your own weights for the provided models, you can also choose to load them by
checking the related option; simply make sure they are compatible with the model you selected.</p>
<p>To import your own model, see : <a class="reference internal" href="custom_model_template.html#custom-model-guide"><span class="std std-ref">Advanced : Declaring a custom model</span></a>, please note this is still a WIP.</p>
</li>
<li><p>Validation proportion : the percentage is listed is how many images will be used for training versus validation.
Validation can work with as little as one image, however performance will greatly improve the more images there are.
Use 90% only if you have a very small dataset (less than 5 images).</p></li>
<li><p>Save as zip : simply copies the results in a zip archive for easier transfer.</p></li>
</ul>
<p>Now, we can switch to the next tab : data augmentation.</p>
<p>If you have cropped cubic images with a power of two as the edge length, you do not need to extract patches,
your images are usable as is.
However, if you are using larger images or with dissimilar sizes,
you can use this option to auto-extract smaller patches that will be automatically padded back to a power
of two no matter the size you choose. For optimal performance, make sure to use a value close or equal to
a power of two still, such as 64 or 120.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Using a too large value for the size will cause memory issues. If this happens, restart napari (better handling for these situations might be added in the future).</p>
</div>
<p>You also have the option to use data augmentation, which can improve performance and generalization.
In most cases this should left enabled.</p>
<p>Finally, the last tab lets you choose :</p>
<ul>
<li><p>The models</p>
<blockquote>
<div><ul class="simple">
<li><p>SegResNet is a lightweight model (low memory requirements) from MONAI originally designed for 3D fMRI data.</p></li>
<li><p>VNet is a larger (than SegResNet) CNN from MONAI designed for medical image segmentation.</p></li>
<li><p>TRAILMAP is our PyTorch implementation of a 3D CNN model trained for axonal detection in cleared tissue.</p></li>
<li><p>TRAILMAP_MS is our implementation in PyTorch additionally trained on mouse cortical neural nuclei from mesoSPIM data.</p></li>
<li><p>Note, the code is very modular, so it is relatively straightforward to use (and contribute) your model as well.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>The loss : for object detection in 3D volumes you’ll likely want to use the Dice or Dice-focal Loss.</p></li>
<li><p>Batch size : chose a value that fits your available memory. If you want to avoid memory issues due to the batch size,
leave it on one.</p></li>
<li><p>Learning rate : if you are not using pre-trained weights or loading your own custom ones, try with 1e-3. Use smaller values
if you are using custom/pre-trained weights.</p></li>
<li><p>Number of epochs : The larger the value, the longer the training will take, but performance might improve with longer
training.  You could start with 40, and see if the loss decreases while the validation metric rises.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>During training, you can monitor the process using the plots : the one on the right (validation) should increase
whereas the loss should decrease. If the validation starts lowering after reaching a maximum, but the loss still decreases,
it could indicate over-fitting, which will negatively impact generalization for the given weights.
You might want use weights generated from the epoch with the maximum validation score if that is the case.</p>
</div>
<div class="figure align-center" id="id5">
<img alt="../../_images/plots_train.png" src="../../_images/plots_train.png" />
<p class="caption"><span class="caption-text">Plots displayed by the training module after 40 epochs</span><a class="headerlink" href="#id5" title="Permalink to this image"></a></p>
</div>
<ul class="simple">
<li><p>Validation interval : if the value is e.g. 2, the training will stop every 2 epochs to perform validation (check performance)
and save the results if the score is better than previously. A larger value will accelerate training, but might cause the saving to miss
better scores. Reasonably, start with 1 for short training sessions (less than 10 epochs) and increase it to two or three if you are training
for 20-60 epochs.</p></li>
<li><p>Deterministic training : if you wish for the training to have reproducibility, enable this and remember the seed you use.
Using the same seed with the same model, images, and parameters should consistently yield similar results. See <a class="reference external" href="https://docs.monai.io/en/stable/utils.html#module-monai.utils.misc">MONAI deterministic training</a>.</p></li>
</ul>
<p>Once all these parameters are set, you can start the training. You can monitor the progress with the plots; should you want to stop
the training you can do so anytime  by pressing the Start button again, whose text should change to <strong>Click to stop</strong>.</p>
<p>In the results folder, you will have access to the weights from training (<strong>.pth</strong> files),
which you can then use in inference.</p>
</div>
<div class="section" id="inference">
<h3>Inference<a class="headerlink" href="#inference" title="Permalink to this heading"></a></h3>
<p>To start, simply choose which folder of images you’d like to run inference on, then the folder in which you’d like the results to be.</p>
<p>Then, select the model you trained (see note below for SegResNet), and load your weights from training.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you already trained a SegResNet, set the counter below the model choice to the size of the images you trained the model on.
(Either use the size of the image itself if you did not extract patches, or the size of the nearest superior power of two of the patches you extracted)</p>
<p>Example :</p>
<ul class="simple">
<li><p>If you used <span class="math notranslate nohighlight">\(64^3\)</span> whole volumes to train the model, enter <span class="math notranslate nohighlight">\(64\)</span> in the counter.</p></li>
<li><p>If you extracted <span class="math notranslate nohighlight">\(120^3\)</span> patches from larger images, enter <span class="math notranslate nohighlight">\(128\)</span></p></li>
</ul>
</div>
<p>Next, you can choose to use window inference, use this if you have very large images.
Please note that using too small of a window might degrade performance, set the size appropriately.</p>
<p>You can also keep the dataset on the CPU to reduce memory usage, but this might slow down the inference process.</p>
<p>If you have anisotropic volumes, you can compensate for it by entering the resolution of your microscope.</p>
<p>By default, inference will calculate and display probability maps (values between 0 and 1).</p>
<p>If you’d like to have semantic labels (only 0 and 1) rather than a probability map, set the thresholding to the desired probability.</p>
<p>If instead you’d prefer to have instance labels, you can enable instance segmentation and select :</p>
<ul>
<li><p>The method</p>
<blockquote>
<div><ul class="simple">
<li><p>Connected components : all separated items with a value above the threshold will be labeled as an instance</p></li>
<li><p>Watershed : objects will be assigned an ID by using the gradient probability at the center of each (set the threshold to a decently high probability for best results).</p></li>
</ul>
</div></blockquote>
</li>
<li><p>The threshold : Objects above this threshold will be retained as single instances.</p></li>
<li><p>Small object removal : Use this to remove small artifacts; all objects below this volume in pixels will be removed.</p></li>
</ul>
<p>Using instance segmentation, you can also analyze the results by checking the <em>Save stats to CSV</em> option.</p>
<p>This will compute :</p>
<ul class="simple">
<li><p>The volume of each cell in pixels</p></li>
<li><p>The centroid coordinates in <span class="math notranslate nohighlight">\(X,Y,Z\)</span></p></li>
<li><p>The sphericity of each cell</p></li>
<li><p>The original size of the image</p></li>
<li><p>The total volume in pixels</p></li>
<li><p>The total volume occupied by objects</p></li>
<li><p>The ratio of <span class="math notranslate nohighlight">\(\frac {Volume_{label}} {Volume_{total}}\)</span></p></li>
<li><p>The total number of unique object instance</p></li>
</ul>
<p>If you wish to see some of the results, you can leave the <em>View results in napari</em> option checked.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you’d like some of these results to be plotted for you, check out the <a class="reference external" href="https://github.com/AdaptiveMotorControlLab/CellSeg3d/tree/main/notebooks">provided notebooks</a></p>
</div>
<p>You can then launch inference and the results will be saved to your specified folder.</p>
<div class="figure align-right" id="id6">
<a class="reference internal image-reference" href="../../_images/plot_example_metrics.png"><img alt="../../_images/plot_example_metrics.png" src="../../_images/plot_example_metrics.png" style="width: 183.6px; height: 438.59999999999997px;" /></a>
<p class="caption"><span class="caption-text">Dice metric score plot</span><a class="headerlink" href="#id6" title="Permalink to this image"></a></p>
</div>
</div>
</div>
<div class="section" id="scoring-review-analysis">
<h2>Scoring, review, analysis<a class="headerlink" href="#scoring-review-analysis" title="Permalink to this heading"></a></h2>
<div class="section" id="model-performance-metrics-utility">
<h3>Model performance : Metrics utility<a class="headerlink" href="#model-performance-metrics-utility" title="Permalink to this heading"></a></h3>
<p>Using the metrics utility module, you can compare the model’s predictions to any ground truth
labels you might have.</p>
<p>Simply provide your prediction and ground truth labels, and compute the results.
A Dice metric of 1 indicates perfect matching, whereas a score of 0 indicates complete mismatch.</p>
<p>Select which score <strong>you consider as sub-optimal</strong>, and all results below this will be <strong>shown in napari</strong>.</p>
<p>If at any time the <strong>orientation of your prediction labels changed compared to the ground truth</strong>, check the
“Find best orientation” option to compensate for it.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using inference on the images you used for training might yield to higher scores due to the fitting.
If you wish to know how well the model performs on more general datasets, use different images for inference and scoring.</p>
</div>
</div>
<div class="section" id="labels-review">
<h3>Labels review<a class="headerlink" href="#labels-review" title="Permalink to this heading"></a></h3>
<p>Using the review module, you can correct the predictions from the model.
Simply load your images and labels, enter the name of the CSV (to keep track of the review process, it will
record which slices have been checked or not and the time taken).</p>
<p>See the <a class="reference external" href="https://napari.org/howtos/layers/labels.html#selecting-a-label">napari tutorial on annotation</a> for instruction on correcting and adding labels.</p>
<p>If you wish to see the surroundings of an object to ensure it should be labeled,
you can use <strong>Shift+Click</strong> on the location you wish to see; this will plot
the  surroundings of the selected location for easier viewing.</p>
<div class="figure align-center" id="id7">
<img alt="../../_images/review_process_example.png" src="../../_images/review_process_example.png" />
<p class="caption"><span class="caption-text">Layout of the review module</span><a class="headerlink" href="#id7" title="Permalink to this image"></a></p>
</div>
<p>Once you are done with the review of a slice, press the “Not checked” button to switch the status to
“Checked” and save the time taken in the csv file.</p>
<p>Finally, when you are done, press the <em>Save</em> button to record your work.</p>
</div>
<div class="section" id="analysis-jupyter-notebooks">
<h3>Analysis : Jupyter notebooks<a class="headerlink" href="#analysis-jupyter-notebooks" title="Permalink to this heading"></a></h3>
<p>In the <a class="reference external" href="https://github.com/AdaptiveMotorControlLab/CellSeg3d/tree/main/notebooks">notebooks folder of the repository</a>, you can find notebooks you can use directly to plot
labels (full_plot.ipynb) or notebooks for plotting the results from your inference csv with object stats (csv_cell_plot.ipynb).</p>
<p>Simply enter your folder or csv file path and the notebooks will plot your results.
Make sure you have all required libraries installed and jupyter extensions set up as explained
for the plots to work.</p>
<div class="figure align-center" id="id8">
<img alt="../../_images/stat_plots.png" src="../../_images/stat_plots.png" />
<p class="caption"><span class="caption-text">Example of the plot present in the notebooks.
Coordinates are based on centroids, the size represents the volume, the color the sphericity.</span><a class="headerlink" href="#id8" title="Permalink to this image"></a></p>
</div>
<p>With this complete, you can repeat the workflow as needed.</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="cropping_module_guide.html" class="btn btn-neutral float-left" title="Cropping utility guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="custom_model_template.html" class="btn btn-neutral float-right" title="Advanced : Declaring a custom model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Cyril Achard, Maxime Vidal.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Inference module guide &mdash; napari-cellseg3d  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Training module guide - Unsupervised models" href="training_module_guide.html" />
    <link rel="prev" title="Review module guide" href="review_module_guide.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html">
            
              <img src="../../_static/logo_alpha.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Main modules guides:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../welcome.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="review_module_guide.html">Review module guide</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Inference module guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#interface-and-functionalities">Interface and functionalities</a></li>
<li class="toctree-l2"><a class="reference internal" href="#wnet">WNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="#source-code">Source code</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="training_module_guide.html">Training module guide - Unsupervised models</a></li>
<li class="toctree-l1"><a class="reference internal" href="training_module_guide.html#training-module-guide-supervised-models">Training module guide - Supervised models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Utilities :</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="metrics_module_guide.html">Metrics utility guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils_module_guide.html">Label conversion utility guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropping_module_guide.html">Cropping utility guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced guides and walk-through:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="training_wnet.html">WNet model training</a></li>
<li class="toctree-l1"><a class="reference internal" href="detailed_walkthrough.html">Detailed walkthrough - Supervised learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_model_template.html">Advanced : Declaring a custom model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Source files:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../code/interface.html">interface.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_base.html">plugin_base.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_review.html">plugin_review.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_review_dock.html">plugin_dock.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_crop.html">plugin_crop.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_convert.html">plugin_convert.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_metrics.html">plugin_metrics.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/model_framework.html">model_framework.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/workers.html">workers.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/instance_segmentation.html">instance_segmentation.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_model_inference.html">plugin_model_inference.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/plugin_model_training.html">plugin_model_training.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code/utils.html">utils.py</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">napari-cellseg3d</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Inference module guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/res/guides/inference_module_guide.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="inference-module-guide">
<span id="id1"></span><h1>Inference module guide<a class="headerlink" href="#inference-module-guide" title="Permalink to this headline"></a></h1>
<p>This module allows you to use pre-trained segmentation algorithms (written in Pytorch) on 3D volumes
to automatically label cells.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Currently, only inference on <strong>3D volumes is supported</strong>. If using folders, your images and labels folders
should both contain a set of <strong>3D image files</strong>, either <strong>.tif</strong> or <strong>.tiff</strong>.
Otherwise you may run inference on layers in napari.</p>
</div>
<p>Currently, the following pre-trained models are available :</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 13%" />
<col style="width: 87%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Link to original paper</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>VNet</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/1606.04797.pdf">Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation</a></p></td>
</tr>
<tr class="row-odd"><td><p>SegResNet</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/1810.11654.pdf">3D MRI brain tumor segmentation using autoencoder regularization</a></p></td>
</tr>
<tr class="row-even"><td><p>TRAILMAP_MS</p></td>
<td><p>A PyTorch implementation of the <a class="reference external" href="https://github.com/AlbertPun/TRAILMAP">TRAILMAP project on GitHub</a> pretrained with mesoSPIM data</p></td>
</tr>
<tr class="row-odd"><td><p>TRAILMAP</p></td>
<td><p>An implementation of the <a class="reference external" href="https://github.com/AlbertPun/TRAILMAP">TRAILMAP project on GitHub</a> using a <a class="reference external" href="https://github.com/wolny/pytorch-3dunet">3DUNet for PyTorch</a></p></td>
</tr>
<tr class="row-even"><td><p>SwinUNetR</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2201.01266">Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images</a></p></td>
</tr>
<tr class="row-odd"><td><p>WNet</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1711.08506">WNet, A Deep Model for Fully Unsupervised Image Segmentation</a></p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For WNet-specific instruction please refer to  the appropriate section below.</p>
</div>
<section id="interface-and-functionalities">
<h2>Interface and functionalities<a class="headerlink" href="#interface-and-functionalities" title="Permalink to this headline"></a></h2>
<a class="reference internal image-reference" href="../../_images/inference_plugin_layout.png"><img alt="../../_images/inference_plugin_layout.png" class="align-right" src="../../_images/inference_plugin_layout.png" style="width: 230.8px; height: 732.8000000000001px;" /></a>
<ul>
<li><p><strong>Loading data</strong> :</p>
<div class="line-block">
<div class="line">When launching the module, you will be asked to provide an <strong>image layer</strong> or an <strong>image folder</strong> with the 3D volumes you’d like to be labeled.</div>
<div class="line">If loading from folder : All images with the chosen extension (<strong>.tif</strong> or <strong>.tiff</strong> currently supported) in this folder will be labeled.</div>
<div class="line">You can then choose an <strong>output folder</strong>, where all the results will be saved.</div>
</div>
</li>
<li><p><strong>Model choice</strong> :</p>
<div class="line-block">
<div class="line">You can then choose one of the provided <strong>models</strong> above, which will be used for inference.</div>
<div class="line">You may also choose to <strong>load custom weights</strong> rather than the pre-trained ones, simply ensure they are <strong>compatible</strong> (e.g. produced from the training module for the same model)</div>
<div class="line">If you choose to use SegResNet or SwinUNetR with custom weights, you will have to provide the size of images it was trained on to ensure compatibility. (See note below)</div>
</div>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Currently the SegResNet and SwinUNetR models requires you to provide the size of the images the model was trained with.
Provided weights use a size of 128, please leave it on the default value if you’re not using custom weights.</p>
</div>
<ul>
<li><p><strong>Inference parameters</strong> :</p>
<div class="line-block">
<div class="line">You can choose to use inference on the whole image at once, which generally yields better performance at the cost of more memory, or you can use a specific window size to run inference on smaller chunks one by one, for lower memory usage.</div>
<div class="line">You can also choose to keep the dataset in the RAM rather than the VRAM (cpu vs cuda device) to avoid running out of VRAM if you have several images.</div>
</div>
</li>
<li><p><strong>Anisotropy</strong> :</p>
<div class="line-block">
<div class="line">If you want to see your results without <strong>anisotropy</strong> when you have anisotropic images, you can specify that you have anisotropic data and set the <strong>resolution of your imaging method in micron</strong>, this wil save and show the results without anisotropy.</div>
</div>
</li>
<li><p><strong>Thresholding</strong> :</p>
<div class="line-block">
<div class="line">You can perform thresholding to <strong>binarize your labels</strong>,</div>
<div class="line">all values beneath the <strong>confidence threshold</strong> will be set to 0 using this.</div>
</div>
</li>
<li><p><strong>Instance segmentation</strong> :</p>
<div class="line-block">
<div class="line">You can convert the semantic segmentation into instance labels by using either the Voronoi-Otsu, <a class="reference external" href="https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_watershed.html">Watershed</a> or <a class="reference external" href="https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.label">Connected Components</a> method, as detailed in <a class="reference internal" href="utils_module_guide.html#utils-module-guide"><span class="std std-ref">Label conversion utility guide</span></a>.</div>
<div class="line">Instance labels will be saved (and shown if applicable) separately from other results.</div>
</div>
</li>
</ul>
<ul>
<li><p><strong>Computing objects statistics</strong> :</p>
<blockquote>
<div><p>You can choose to compute various stats from the labels and save them to a .csv for later use.</p>
<p>This includes, for each object :</p>
<ul class="simple">
<li><p>Object volume (pixels)</p></li>
<li><p><span class="math notranslate nohighlight">\(X,Y,Z\)</span> coordinates of the centroid</p></li>
<li><p>Sphericity</p></li>
</ul>
<p>And more general statistics :</p>
<ul class="simple">
<li><p>Image size</p></li>
<li><p>Total image volume (pixels)</p></li>
<li><p>Total object (labeled) volume (pixels)</p></li>
<li><p>Filling ratio (fraction of the volume that is labeled)</p></li>
<li><p>The number of labeled objects</p></li>
</ul>
<p>In the <code class="docutils literal notranslate"><span class="pre">notebooks</span></code> folder you can find an example of plotting cell statistics using the result csv.</p>
</div></blockquote>
</li>
</ul>
<p>When you are done choosing your parameters, you can press the <strong>Start</strong> button to begin the inference process.
Once it has finished, results will be saved then displayed in napari; each output will be paired with its original.
On the left side, a progress bar and a log will keep you informed on the process.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line">The files will be saved using the following format :</div>
<div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">{original_name}_{model}_{date</span> <span class="pre">&amp;</span> <span class="pre">time}_pred{id}.file_ext</span></code></div>
</div>
<div class="line">For example, using a VNet on the third image of a folder, called “somatomotor.tif” will yield the following name :</div>
<div class="line-block">
<div class="line"><em>somatomotor_VNet_2022_04_06_15_49_42_pred3.tif</em></div>
</div>
<div class="line">Instance labels will have the “Instance_seg” prefix appended to the name.</div>
</div>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<div class="line-block">
<div class="line"><strong>Results</strong> will be displayed using the <strong>twilight shifted</strong> colormap if raw or <strong>turbo</strong> if thresholding has been applied, whereas the <strong>original</strong> image will be shown in the <strong>inferno</strong> colormap.</div>
<div class="line">Feel free to change the <strong>colormap</strong> or <strong>contrast</strong> when viewing results to ensure you can properly see the labels.</div>
<div class="line">You’ll most likely want to use <strong>3D view</strong> and <strong>grid mode</strong> in napari when checking results more broadly.</div>
</div>
</div>
<img alt="../../_images/inference_results_example.png" src="../../_images/inference_results_example.png" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can save the log after the worker is finished to easily remember which parameters you ran inference with.</p>
</div>
</section>
<section id="wnet">
<h2>WNet<a class="headerlink" href="#wnet" title="Permalink to this headline"></a></h2>
<p>The WNet model, from the paper <a class="reference external" href="https://arxiv.org/abs/1711.08506">WNet, A Deep Model for Fully Unsupervised Image Segmentation</a>, is a fully unsupervised model that can be used to segment images without any labels.
It clusters pixels based on brightness, and can be used to segment cells in a variety of modalities.
Its use and available options are similar to the above models, with a few differences :</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line">Our provided, pre-trained model should use an input size of 64x64x64. As such, window inference is always enabled</div>
<div class="line">and set to 64. If you want to use a different size, you will have to train your own model using the provided notebook.</div>
</div>
</div>
<p>All it requires are 3D .tif images (you can also load a 2D stack as 3D via napari).</p>
</section>
<section id="source-code">
<h2>Source code<a class="headerlink" href="#source-code" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="../code/plugin_model_inference.html"><span class="doc">plugin_model_inference.py</span></a></p></li>
<li><p><a class="reference internal" href="../code/model_framework.html"><span class="doc">model_framework.py</span></a></p></li>
<li><p><a class="reference internal" href="../code/workers.html"><span class="doc">workers.py</span></a></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="review_module_guide.html" class="btn btn-neutral float-left" title="Review module guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="training_module_guide.html" class="btn btn-neutral float-right" title="Training module guide - Unsupervised models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022-2023, Cyril Achard, Maxime Vidal.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>